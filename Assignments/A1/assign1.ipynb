{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12468a0",
   "metadata": {},
   "source": [
    "## A1_Thats What I LIKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3487fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b1fc6",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a61240",
   "metadata": {},
   "source": [
    "Modify the Word2Vec (with & without negative sampling) and GloVe from the lab lecture (3 points)\n",
    "\n",
    "\n",
    "• Train using a real-world corpus (suggest to categories news from nltk datset). Ensure to source\n",
    "this dataset from reputable public databases or repositories. It is imperative to give proper\n",
    "credit to the dataset source in your documentation.\n",
    "\n",
    "\n",
    "• Create a function that allows dynamic modification of the window size during training. Use a\n",
    "window size of 2 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547a388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e2f6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing news as suggested\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aaccba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus = brown.sents(categories='news')\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = news_corpus[:300] # taking small subset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16290b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(news_corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e6605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef85ec01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help', 'plus', 'L.', '1961']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe9e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for (i, w) in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8e71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5eabc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocabs)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46455180",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fbe11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = word2index[sent[target_index]]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(word2index[sent[target_index - count]])\n",
    "                context.append(word2index[sent[target_index + count]])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append([target, word])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea009616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1343]\n",
      " [ 775]]\n",
      "Target:  [[1400]\n",
      " [1439]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd40885",
   "metadata": {},
   "source": [
    "#### Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "848c0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "\n",
    "        self.embedding_center   = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_outside  = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center, outside, all_vocab):\n",
    "        center_embedding    = self.embedding_center(center)  \n",
    "        outside_embedding   = self.embedding_outside(outside)   # (batch_size, 1, embedding_size)\n",
    "        all_vocab_embedding = self.embedding_outside(all_vocab) # (batch_size, vocab_size, embedding_size)\n",
    "\n",
    "        top_term   = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        # (b_size, 1, emb_size) @ (b_size, emb_size, 1) = (b_size, 1, 1) -> (b_size, 1)\n",
    "\n",
    "\n",
    "        down_term = all_vocab_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        # (b_size, vocab_size, emb_size) @ (b_size, emb_size, 1) = (b_size, vocab_size, 1) -> (b_size, vocab_size)\n",
    "\n",
    "        denominator_sum = torch.sum(torch.exp(down_term), 1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(top_term / denominator_sum)) # scalar\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc782315",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2\n",
    "model          = Skipgram(vocab_size, embedding_size)\n",
    "model_skipgram = model.to('cpu')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f55e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w):\n",
    "    if word2index.get(w) is not None:\n",
    "        return word2index[w]\n",
    "    else:\n",
    "        return word2index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f47012d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1939])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, len(vocabs))  # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd105c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd13dd",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "968d1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 9.753088 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 7.537870 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 7.910213 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 8.303257 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 7.982500 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    # no cuda, so cpu \n",
    "    input_batch  = input_batch.to(\"cpu\")\n",
    "    target_batch = target_batch.to(\"cpu\")\n",
    "    all_vocabs   = all_vocabs.to(\"cpu\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_skipgram(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5f571d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_skipgram.state_dict(), 'model/skipgram_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c339efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_skipgram, open('model/skipgram.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20cae4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vocabs[0]\n",
    "id = word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ac348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.LongTensor([id])\n",
    "id_tensor = id_tensor.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3ffe8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5500, -0.6442]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-1.6433,  0.1288]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embedding by averaging\n",
    "v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "u_embed = model_skipgram.embedding_outside(id_tensor)\n",
    "\n",
    "v_embed, u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec739719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2577, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average to get the word embedding\n",
    "word_embed = (v_embed + u_embed) / 2\n",
    "word_embed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc813a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skip_gram(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(\"cpu\")\n",
    "    v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "    u_embed = model_skipgram.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2ddcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da2bc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_skip_gram('government')\n",
    "officials = get_embed_skip_gram('officials')\n",
    "administration = get_embed_skip_gram('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "347fb3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: 0.6091\n",
      "government vs administration: 0.9920\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31388b",
   "metadata": {},
   "source": [
    "### Skipgram Negative Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "643818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57ca17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_flatten = flatten(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3402c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6642"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = Counter(news_flatten)\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ceddd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocabs:\n",
    "    uw = word_count[vo] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / Z)\n",
    "    unigram_table.extend([vo] * uw_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a525b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 113,\n",
       "         ',': 93,\n",
       "         '.': 87,\n",
       "         'of': 77,\n",
       "         'to': 64,\n",
       "         'a': 53,\n",
       "         'in': 45,\n",
       "         'and': 45,\n",
       "         '``': 36,\n",
       "         \"''\": 36,\n",
       "         'for': 36,\n",
       "         'The': 31,\n",
       "         'said': 27,\n",
       "         'would': 27,\n",
       "         'that': 25,\n",
       "         'by': 25,\n",
       "         'on': 22,\n",
       "         'be': 22,\n",
       "         'was': 22,\n",
       "         'is': 19,\n",
       "         'as': 17,\n",
       "         'which': 15,\n",
       "         'will': 15,\n",
       "         'it': 15,\n",
       "         'he': 15,\n",
       "         'his': 14,\n",
       "         'at': 13,\n",
       "         'Texas': 13,\n",
       "         'year': 13,\n",
       "         'jury': 13,\n",
       "         'who': 13,\n",
       "         '--': 12,\n",
       "         'an': 12,\n",
       "         'not': 12,\n",
       "         'with': 11,\n",
       "         'million': 11,\n",
       "         'bill': 11,\n",
       "         'has': 11,\n",
       "         'from': 11,\n",
       "         'plan': 11,\n",
       "         'election': 11,\n",
       "         'House': 11,\n",
       "         'this': 10,\n",
       "         'schools': 10,\n",
       "         'or': 10,\n",
       "         'are': 10,\n",
       "         'medical': 10,\n",
       "         'He': 9,\n",
       "         'Dallas': 9,\n",
       "         'been': 9,\n",
       "         'pay': 9,\n",
       "         'President': 9,\n",
       "         'Fulton': 9,\n",
       "         'State': 9,\n",
       "         'other': 9,\n",
       "         'County': 9,\n",
       "         'It': 9,\n",
       "         'care': 8,\n",
       "         'had': 8,\n",
       "         'also': 8,\n",
       "         'there': 8,\n",
       "         'new': 8,\n",
       "         'more': 8,\n",
       "         'state': 8,\n",
       "         'school': 8,\n",
       "         'federal': 8,\n",
       "         'cases': 8,\n",
       "         'under': 8,\n",
       "         'up': 8,\n",
       "         'first': 8,\n",
       "         'have': 8,\n",
       "         'grants': 8,\n",
       "         'program': 8,\n",
       "         'resolution': 7,\n",
       "         '1': 7,\n",
       "         'out': 7,\n",
       "         'some': 7,\n",
       "         'one': 7,\n",
       "         'made': 7,\n",
       "         'bonds': 7,\n",
       "         'funds': 7,\n",
       "         'were': 7,\n",
       "         'A': 7,\n",
       "         'committee': 7,\n",
       "         ')': 7,\n",
       "         'tax': 7,\n",
       "         'last': 7,\n",
       "         'home': 7,\n",
       "         'passed': 7,\n",
       "         'dollars': 7,\n",
       "         'its': 7,\n",
       "         'court': 7,\n",
       "         'per': 7,\n",
       "         'county': 7,\n",
       "         '(': 7,\n",
       "         'these': 6,\n",
       "         'Department': 6,\n",
       "         'Rep.': 6,\n",
       "         'Sen.': 6,\n",
       "         'each': 6,\n",
       "         'increase': 6,\n",
       "         'ADC': 6,\n",
       "         'health': 6,\n",
       "         'should': 6,\n",
       "         'them': 6,\n",
       "         'public': 5,\n",
       "         'but': 5,\n",
       "         'told': 5,\n",
       "         'College': 5,\n",
       "         'go': 5,\n",
       "         'did': 5,\n",
       "         'take': 5,\n",
       "         'two': 5,\n",
       "         'research': 5,\n",
       "         'proposal': 5,\n",
       "         'cent': 5,\n",
       "         'trial': 5,\n",
       "         \"'\": 5,\n",
       "         'legislators': 5,\n",
       "         'expected': 5,\n",
       "         'proposed': 5,\n",
       "         'Austin': 5,\n",
       "         'Senate': 5,\n",
       "         'grand': 5,\n",
       "         'In': 5,\n",
       "         'such': 5,\n",
       "         'bills': 5,\n",
       "         'This': 5,\n",
       "         'Kennedy': 5,\n",
       "         'Congress': 5,\n",
       "         'over': 5,\n",
       "         'their': 5,\n",
       "         'I': 5,\n",
       "         'Clark': 5,\n",
       "         'Dr.': 5,\n",
       "         'race': 5,\n",
       "         'security': 5,\n",
       "         'than': 5,\n",
       "         'Legislature': 5,\n",
       "         'vote': 5,\n",
       "         'general': 5,\n",
       "         'they': 5,\n",
       "         'now': 5,\n",
       "         '10': 5,\n",
       "         'no': 5,\n",
       "         'dental': 5,\n",
       "         'ward': 5,\n",
       "         'when': 5,\n",
       "         'work': 5,\n",
       "         'property': 5,\n",
       "         'social': 5,\n",
       "         'payroll': 5,\n",
       "         'city': 5,\n",
       "         'day': 5,\n",
       "         'asked': 5,\n",
       "         'make': 5,\n",
       "         'cost': 5,\n",
       "         'Monday': 5,\n",
       "         'law': 5,\n",
       "         'people': 5,\n",
       "         'years': 5,\n",
       "         'Karns': 5,\n",
       "         'days': 5,\n",
       "         'persons': 5,\n",
       "         'Wexler': 5,\n",
       "         'being': 4,\n",
       "         'taxes': 4,\n",
       "         'help': 4,\n",
       "         'defendants': 4,\n",
       "         'provide': 4,\n",
       "         'costs': 4,\n",
       "         'precinct': 4,\n",
       "         'teacher': 4,\n",
       "         'voters': 4,\n",
       "         'Education': 4,\n",
       "         'after': 4,\n",
       "         'water': 4,\n",
       "         'Bellows': 4,\n",
       "         'rural': 4,\n",
       "         'special': 4,\n",
       "         'involved': 4,\n",
       "         'all': 4,\n",
       "         'case': 4,\n",
       "         'Thursday': 4,\n",
       "         'counties': 4,\n",
       "         'time': 4,\n",
       "         'Hartsfield': 4,\n",
       "         'given': 4,\n",
       "         'campaign': 4,\n",
       "         'approved': 4,\n",
       "         'B.': 4,\n",
       "         'J.': 4,\n",
       "         ':': 4,\n",
       "         'issue': 4,\n",
       "         'Atlanta': 4,\n",
       "         'Williams': 4,\n",
       "         'recommended': 4,\n",
       "         'deaf': 4,\n",
       "         'nursing': 4,\n",
       "         'since': 4,\n",
       "         'report': 4,\n",
       "         'hospital': 4,\n",
       "         'Committee': 4,\n",
       "         'Washington': 4,\n",
       "         'Republicans': 4,\n",
       "         'against': 4,\n",
       "         'Parkhouse': 4,\n",
       "         'Judge': 4,\n",
       "         'received': 4,\n",
       "         'most': 4,\n",
       "         'statements': 4,\n",
       "         'City': 4,\n",
       "         'Highway': 4,\n",
       "         'candidate': 4,\n",
       "         'Pelham': 4,\n",
       "         'aged': 4,\n",
       "         'act': 4,\n",
       "         'night': 4,\n",
       "         'like': 4,\n",
       "         'give': 3,\n",
       "         'welfare': 3,\n",
       "         'whether': 3,\n",
       "         'hearing': 3,\n",
       "         'Roberts': 3,\n",
       "         'Parsons': 3,\n",
       "         'Criminal': 3,\n",
       "         'called': 3,\n",
       "         'Jr.': 3,\n",
       "         'judges': 3,\n",
       "         'because': 3,\n",
       "         'cities': 3,\n",
       "         'about': 3,\n",
       "         'scheduled': 3,\n",
       "         'fair': 3,\n",
       "         'money': 3,\n",
       "         'Friday': 3,\n",
       "         'next': 3,\n",
       "         'constituted': 3,\n",
       "         'attorney': 3,\n",
       "         'semester': 3,\n",
       "         'Vandiver': 3,\n",
       "         'precincts': 3,\n",
       "         'White': 3,\n",
       "         '13': 3,\n",
       "         'employment': 3,\n",
       "         'any': 3,\n",
       "         'District': 3,\n",
       "         'irregularities': 3,\n",
       "         'worth': 3,\n",
       "         'present': 3,\n",
       "         '65': 3,\n",
       "         'could': 3,\n",
       "         'may': 3,\n",
       "         'children': 3,\n",
       "         'George': 3,\n",
       "         'patient': 3,\n",
       "         'governor': 3,\n",
       "         'between': 3,\n",
       "         'amount': 3,\n",
       "         'Cook': 3,\n",
       "         'seven': 3,\n",
       "         'estimated': 3,\n",
       "         'age': 3,\n",
       "         'upon': 3,\n",
       "         '20': 3,\n",
       "         'further': 3,\n",
       "         \"Georgia's\": 3,\n",
       "         '2': 3,\n",
       "         'former': 3,\n",
       "         'primary': 3,\n",
       "         'insurance': 3,\n",
       "         'courses': 3,\n",
       "         'political': 3,\n",
       "         'means': 3,\n",
       "         'doctor': 3,\n",
       "         'into': 3,\n",
       "         'railroad': 3,\n",
       "         'services': 3,\n",
       "         'similar': 3,\n",
       "         'proposals': 3,\n",
       "         'anonymous': 3,\n",
       "         'dollar': 3,\n",
       "         '23d': 3,\n",
       "         'Sam': 3,\n",
       "         'When': 3,\n",
       "         'listed': 3,\n",
       "         'session': 3,\n",
       "         'chairman': 3,\n",
       "         'retirement': 3,\n",
       "         'taken': 3,\n",
       "         'attend': 3,\n",
       "         'There': 3,\n",
       "         'administration': 3,\n",
       "         'Daniel': 3,\n",
       "         'calls': 3,\n",
       "         'recommendations': 3,\n",
       "         'long': 3,\n",
       "         'Grover': 3,\n",
       "         'discrimination': 3,\n",
       "         'meet': 3,\n",
       "         'Berry': 3,\n",
       "         'got': 3,\n",
       "         '4': 3,\n",
       "         'must': 3,\n",
       "         'education': 3,\n",
       "         'argued': 3,\n",
       "         'number': 3,\n",
       "         'degree': 3,\n",
       "         'later': 3,\n",
       "         'companies': 3,\n",
       "         'yesterday': 3,\n",
       "         'local': 3,\n",
       "         'term': 3,\n",
       "         'Barber': 3,\n",
       "         'college': 3,\n",
       "         'Davis': 3,\n",
       "         'homes': 3,\n",
       "         'Executive': 3,\n",
       "         'aid': 3,\n",
       "         'still': 3,\n",
       "         'Georgia': 3,\n",
       "         'Fort': 3,\n",
       "         'amendment': 3,\n",
       "         'place': 3,\n",
       "         'action': 3,\n",
       "         'board': 3,\n",
       "         'none': 3,\n",
       "         'paid': 3,\n",
       "         'authority': 3,\n",
       "         'Ratcliff': 3,\n",
       "         'receive': 3,\n",
       "         \"mayor's\": 3,\n",
       "         'high': 3,\n",
       "         'hours': 3,\n",
       "         'family': 3,\n",
       "         'bankers': 3,\n",
       "         'if': 3,\n",
       "         'those': 3,\n",
       "         'Hospital': 3,\n",
       "         'Martin': 3,\n",
       "         'His': 3,\n",
       "         'polls': 3,\n",
       "         'School': 3,\n",
       "         'evidence': 3,\n",
       "         'elected': 3,\n",
       "         'banks': 3,\n",
       "         'added': 3,\n",
       "         'But': 3,\n",
       "         '24': 3,\n",
       "         'our': 3,\n",
       "         'career': 3,\n",
       "         'fight': 3,\n",
       "         'billion': 3,\n",
       "         'votes': 3,\n",
       "         'programs': 3,\n",
       "         'superintendent': 3,\n",
       "         'constitutional': 3,\n",
       "         'petition': 3,\n",
       "         'Jan.': 3,\n",
       "         'These': 3,\n",
       "         'millions': 3,\n",
       "         'citizens': 3,\n",
       "         'M.': 3,\n",
       "         'child': 3,\n",
       "         'medicine': 3,\n",
       "         'before': 3,\n",
       "         'much': 3,\n",
       "         'many': 3,\n",
       "         'states': 3,\n",
       "         'Court': 3,\n",
       "         'address': 3,\n",
       "         'system': 3,\n",
       "         'E.': 3,\n",
       "         'Paris': 3,\n",
       "         'very': 3,\n",
       "         'spent': 3,\n",
       "         'highway': 3,\n",
       "         'construction': 3,\n",
       "         'practices': 3,\n",
       "         'designed': 3,\n",
       "         'need': 3,\n",
       "         'James': 3,\n",
       "         'voted': 3,\n",
       "         'ones': 3,\n",
       "         'ballot': 3,\n",
       "         'get': 3,\n",
       "         'reduce': 3,\n",
       "         'soon': 3,\n",
       "         'causes': 3,\n",
       "         '30': 3,\n",
       "         'Gov.': 3,\n",
       "         'bond': 3,\n",
       "         'needs': 3,\n",
       "         'members': 3,\n",
       "         'both': 3,\n",
       "         'Oklahoma': 3,\n",
       "         'charged': 3,\n",
       "         'investigation': 3,\n",
       "         'enabling': 3,\n",
       "         'force': 3,\n",
       "         'Houston': 3,\n",
       "         'principal': 3,\n",
       "         'Charles': 3,\n",
       "         'message': 3,\n",
       "         'Bush': 3,\n",
       "         '3': 3,\n",
       "         'can': 3,\n",
       "         'Cotten': 3,\n",
       "         'future': 3,\n",
       "         'establishment': 3,\n",
       "         'teaching': 3,\n",
       "         'scholarships': 3,\n",
       "         'through': 3,\n",
       "         'annual': 3,\n",
       "         'greater': 2,\n",
       "         'details': 2,\n",
       "         'approve': 2,\n",
       "         'director': 2,\n",
       "         'Other': 2,\n",
       "         'real': 2,\n",
       "         'possible': 2,\n",
       "         'Under': 2,\n",
       "         'Hays': 2,\n",
       "         'run': 2,\n",
       "         'addition': 2,\n",
       "         'Association': 2,\n",
       "         'scholarship': 2,\n",
       "         'involving': 2,\n",
       "         'do': 2,\n",
       "         'airport': 2,\n",
       "         'except': 2,\n",
       "         'Worth': 2,\n",
       "         'American': 2,\n",
       "         'where': 2,\n",
       "         'little': 2,\n",
       "         'lacking': 2,\n",
       "         \"Atlanta's\": 2,\n",
       "         'confidence': 2,\n",
       "         'grant': 2,\n",
       "         'Allen': 2,\n",
       "         'escheat': 2,\n",
       "         'among': 2,\n",
       "         'fund': 2,\n",
       "         'Ordinary': 2,\n",
       "         'roads': 2,\n",
       "         'sign': 2,\n",
       "         'enough': 2,\n",
       "         'veiled': 2,\n",
       "         'Kan.': 2,\n",
       "         '8': 2,\n",
       "         'today': 2,\n",
       "         'paying': 2,\n",
       "         'several': 2,\n",
       "         'Authority': 2,\n",
       "         'president': 2,\n",
       "         'veteran': 2,\n",
       "         'required': 2,\n",
       "         'apparently': 2,\n",
       "         'meeting': 2,\n",
       "         'raises': 2,\n",
       "         'then': 2,\n",
       "         'bit': 2,\n",
       "         'Mayor': 2,\n",
       "         'betting': 2,\n",
       "         'audience': 2,\n",
       "         'urged': 2,\n",
       "         'defeated': 2,\n",
       "         'adjournment': 2,\n",
       "         'something': 2,\n",
       "         'jail': 2,\n",
       "         'Blue': 2,\n",
       "         'Rural': 2,\n",
       "         'Denton': 2,\n",
       "         'Ridge': 2,\n",
       "         'interest': 2,\n",
       "         'expense': 2,\n",
       "         'student': 2,\n",
       "         'legislator': 2,\n",
       "         '&': 2,\n",
       "         'follow': 2,\n",
       "         'fire': 2,\n",
       "         'assistance': 2,\n",
       "         'big': 2,\n",
       "         'personnel': 2,\n",
       "         'advisement': 2,\n",
       "         'Bill': 2,\n",
       "         'modest': 2,\n",
       "         'crisis': 2,\n",
       "         '50': 2,\n",
       "         'Full': 2,\n",
       "         'directed': 2,\n",
       "         'mayor': 2,\n",
       "         'Paso': 2,\n",
       "         'eliminating': 2,\n",
       "         'reports': 2,\n",
       "         'They': 2,\n",
       "         'While': 2,\n",
       "         'making': 2,\n",
       "         'suit': 2,\n",
       "         'hospitals': 2,\n",
       "         'date': 2,\n",
       "         'occupation': 2,\n",
       "         'Scott': 2,\n",
       "         'brokers': 2,\n",
       "         'D.': 2,\n",
       "         'cover': 2,\n",
       "         'workers': 2,\n",
       "         'portion': 2,\n",
       "         'previously': 2,\n",
       "         '1963': 2,\n",
       "         'permit': 2,\n",
       "         'W.': 2,\n",
       "         'saw': 2,\n",
       "         'Sept.': 2,\n",
       "         'rescind': 2,\n",
       "         'telephone': 2,\n",
       "         'three': 2,\n",
       "         'sponsor': 2,\n",
       "         'previous': 2,\n",
       "         'pipeline': 2,\n",
       "         'resigned': 2,\n",
       "         'toward': 2,\n",
       "         'deputies': 2,\n",
       "         \"didn't\": 2,\n",
       "         'Paradise': 2,\n",
       "         'Feb.': 2,\n",
       "         'including': 2,\n",
       "         'another': 2,\n",
       "         'charge': 2,\n",
       "         'finally': 2,\n",
       "         'prison': 2,\n",
       "         'couple': 2,\n",
       "         'schooling': 2,\n",
       "         'order': 2,\n",
       "         'my': 2,\n",
       "         'earlier': 2,\n",
       "         'sponsored': 2,\n",
       "         'effort': 2,\n",
       "         'own': 2,\n",
       "         'ever': 2,\n",
       "         \"taxpayers'\": 2,\n",
       "         'Colquitt': 2,\n",
       "         'Felix': 2,\n",
       "         'One': 2,\n",
       "         'debate': 2,\n",
       "         'populous': 2,\n",
       "         'After': 2,\n",
       "         'payment': 2,\n",
       "         'senator': 2,\n",
       "         'Weatherford': 2,\n",
       "         'March': 2,\n",
       "         'dependency': 2,\n",
       "         'departments': 2,\n",
       "         'yet': 2,\n",
       "         'problem': 2,\n",
       "         'coolest': 2,\n",
       "         'Health': 2,\n",
       "         'Calls': 2,\n",
       "         'eight': 2,\n",
       "         'Jones': 2,\n",
       "         'Berlin': 2,\n",
       "         'poll': 2,\n",
       "         'better': 2,\n",
       "         'speaker': 2,\n",
       "         'methods': 2,\n",
       "         'set': 2,\n",
       "         'During': 2,\n",
       "         'Two': 2,\n",
       "         'heard': 2,\n",
       "         'matching': 2,\n",
       "         'revolving': 2,\n",
       "         'opposed': 2,\n",
       "         'bank': 2,\n",
       "         'disclosure': 2,\n",
       "         'Ivan': 2,\n",
       "         'employed': 2,\n",
       "         'reported': 2,\n",
       "         'does': 2,\n",
       "         'took': 2,\n",
       "         'appointment': 2,\n",
       "         'obtain': 2,\n",
       "         'certificate': 2,\n",
       "         'carry': 2,\n",
       "         '1961-62': 2,\n",
       "         'unspecified': 2,\n",
       "         'use': 2,\n",
       "         'commented': 2,\n",
       "         'districts': 2,\n",
       "         'boost': 2,\n",
       "         'candidates': 2,\n",
       "         'require': 2,\n",
       "         'University': 2,\n",
       "         'Research': 2,\n",
       "         'obtained': 2,\n",
       "         'laws': 2,\n",
       "         'drafts': 2,\n",
       "         'personally': 2,\n",
       "         'succeeded': 2,\n",
       "         'U.S.': 2,\n",
       "         '?': 2,\n",
       "         'measure': 2,\n",
       "         'racial': 2,\n",
       "         \"Department's\": 2,\n",
       "         'William': 2,\n",
       "         'held': 2,\n",
       "         'El': 2,\n",
       "         'less': 2,\n",
       "         'credit': 2,\n",
       "         'teachers': 2,\n",
       "         'serious': 2,\n",
       "         'representing': 2,\n",
       "         'end': 2,\n",
       "         'providing': 2,\n",
       "         'You': 2,\n",
       "         'plans': 2,\n",
       "         'hear': 2,\n",
       "         'back': 2,\n",
       "         'opposition': 2,\n",
       "         'petitions': 2,\n",
       "         'approval': 2,\n",
       "         'retailers': 2,\n",
       "         'fine': 2,\n",
       "         'large': 2,\n",
       "         'horse': 2,\n",
       "         'Superior': 2,\n",
       "         'costly': 2,\n",
       "         'says': 2,\n",
       "         'Chairman': 2,\n",
       "         'trouble': 2,\n",
       "         'chief': 2,\n",
       "         'himself': 2,\n",
       "         'government': 2,\n",
       "         'Halleck': 2,\n",
       "         'Republican': 2,\n",
       "         'stocks': 2,\n",
       "         'priority': 2,\n",
       "         'John': 2,\n",
       "         'representatives': 2,\n",
       "         'jurors': 2,\n",
       "         'Aug.': 2,\n",
       "         \"President's\": 2,\n",
       "         '$37': 2,\n",
       "         'brought': 2,\n",
       "         'Officials': 2,\n",
       "         'prejudicial': 2,\n",
       "         'Chapman': 2,\n",
       "         'Cox': 2,\n",
       "         'Science': 2,\n",
       "         'generally': 2,\n",
       "         '$1,000': 2,\n",
       "         'courts': 2,\n",
       "         'denied': 2,\n",
       "         'replied': 2,\n",
       "         'four': 2,\n",
       "         'source': 2,\n",
       "         'Louis': 2,\n",
       "         'say': 2,\n",
       "         'ask': 2,\n",
       "         'you': 2,\n",
       "         'Caldwell': 2,\n",
       "         'Sunday': 2,\n",
       "         'Leader': 2,\n",
       "         'students': 2,\n",
       "         'additional': 2,\n",
       "         'notice': 2,\n",
       "         'combined': 2,\n",
       "         'client': 2,\n",
       "         '$1,500': 2,\n",
       "         'controversy': 2,\n",
       "         'authorizing': 2,\n",
       "         'contracts': 2,\n",
       "         'privilege': 2,\n",
       "         'studied': 2,\n",
       "         'cannot': 2,\n",
       "         'Roads': 2,\n",
       "         '$10': 2,\n",
       "         'unit': 2,\n",
       "         'week': 2,\n",
       "         'rejected': 2,\n",
       "         'requirement': 2,\n",
       "         'test': 2,\n",
       "         '4-year': 2,\n",
       "         'savings': 2,\n",
       "         'C.': 2,\n",
       "         'A.': 2,\n",
       "         'voting': 2,\n",
       "         'choice': 2,\n",
       "         'solve': 2,\n",
       "         'leading': 2,\n",
       "         'illness': 2,\n",
       "         'sp.': 2,\n",
       "         'impossible': 2,\n",
       "         'served': 2,\n",
       "         'places': 2,\n",
       "         'guilt': 2,\n",
       "         'exception': 2,\n",
       "         'violation': 2,\n",
       "         'study': 2,\n",
       "         'Henry': 2,\n",
       "         '58th': 2,\n",
       "         'Welfare': 2,\n",
       "         'Joe': 2,\n",
       "         'we': 2,\n",
       "         'elaborate': 2,\n",
       "         'operated': 2,\n",
       "         \"year's\": 2,\n",
       "         'R.': 2,\n",
       "         'Miller': 2,\n",
       "         'issued': 2,\n",
       "         'sought': 2,\n",
       "         'benefits': 2,\n",
       "         'only': 2,\n",
       "         'San': 2,\n",
       "         'Griffin': 2,\n",
       "         'wife': 2,\n",
       "         'building': 2,\n",
       "         'offer': 2,\n",
       "         'legislation': 2,\n",
       "         'worked': 2,\n",
       "         'person': 2,\n",
       "         'what': 2,\n",
       "         'went': 2,\n",
       "         'however': 2,\n",
       "         'enter': 2,\n",
       "         'getting': 2,\n",
       "         'recommend': 2,\n",
       "         'gas': 2,\n",
       "         'aside': 2,\n",
       "         'orderly': 2,\n",
       "         'community': 2,\n",
       "         'Salinger': 2,\n",
       "         'Sherman': 2,\n",
       "         'problems': 2,\n",
       "         'load': 2,\n",
       "         'so': 2,\n",
       "         'budget': 2,\n",
       "         'build': 2,\n",
       "         'five': 2,\n",
       "         'largest': 2,\n",
       "         'manner': 2,\n",
       "         'April': 2,\n",
       "         'facilities': 2,\n",
       "         'Tuesday': 2,\n",
       "         '6': 2,\n",
       "         'encouragement': 1,\n",
       "         'phone': 1,\n",
       "         'inadequate': 1,\n",
       "         'Technology': 1,\n",
       "         'financed': 1,\n",
       "         'granted': 1,\n",
       "         'sitting': 1,\n",
       "         '4.4': 1,\n",
       "         'county-wide': 1,\n",
       "         'acquire': 1,\n",
       "         'hospital-care': 1,\n",
       "         'doubt': 1,\n",
       "         'entail': 1,\n",
       "         'experienced': 1,\n",
       "         'accounts': 1,\n",
       "         'awarding': 1,\n",
       "         'summer': 1,\n",
       "         'calmest': 1,\n",
       "         'Many': 1,\n",
       "         'Bankers': 1,\n",
       "         'Without': 1,\n",
       "         '33d': 1,\n",
       "         'dismissed': 1,\n",
       "         'term-end': 1,\n",
       "         'culminating': 1,\n",
       "         'staff': 1,\n",
       "         'books': 1,\n",
       "         '21': 1,\n",
       "         'here': 1,\n",
       "         'leaned': 1,\n",
       "         'expand': 1,\n",
       "         'officials': 1,\n",
       "         'Clements': 1,\n",
       "         \"court's\": 1,\n",
       "         'favorable': 1,\n",
       "         'prelude': 1,\n",
       "         'serving': 1,\n",
       "         'limited': 1,\n",
       "         'past': 1,\n",
       "         'Failure': 1,\n",
       "         'traditional': 1,\n",
       "         'ordinary': 1,\n",
       "         'Five': 1,\n",
       "         'If': 1,\n",
       "         'Corps': 1,\n",
       "         'text': 1,\n",
       "         'doing': 1,\n",
       "         'married': 1,\n",
       "         'scholastics': 1,\n",
       "         'Signal': 1,\n",
       "         'probably': 1,\n",
       "         'GOP': 1,\n",
       "         'caused': 1,\n",
       "         'Pierre': 1,\n",
       "         'unanimous': 1,\n",
       "         'skills': 1,\n",
       "         'entering': 1,\n",
       "         'York': 1,\n",
       "         'Dirksen': 1,\n",
       "         'sites': 1,\n",
       "         'outpatient': 1,\n",
       "         'Virgil': 1,\n",
       "         '540': 1,\n",
       "         'over-all': 1,\n",
       "         'succeed': 1,\n",
       "         'tied': 1,\n",
       "         'Mayor-nominate': 1,\n",
       "         'Dumas': 1,\n",
       "         'appraisers': 1,\n",
       "         'Pye': 1,\n",
       "         '$50': 1,\n",
       "         '47': 1,\n",
       "         'right': 1,\n",
       "         '31st': 1,\n",
       "         'areas': 1,\n",
       "         'newly': 1,\n",
       "         'enacted': 1,\n",
       "         'pertained': 1,\n",
       "         'Co.': 1,\n",
       "         'filed': 1,\n",
       "         'so-called': 1,\n",
       "         'arrest': 1,\n",
       "         'allow': 1,\n",
       "         'witnesses': 1,\n",
       "         'Instead': 1,\n",
       "         'praised': 1,\n",
       "         'gifts': 1,\n",
       "         'hailed': 1,\n",
       "         'Several': 1,\n",
       "         'registered': 1,\n",
       "         '5.1': 1,\n",
       "         'coordinator': 1,\n",
       "         'based': 1,\n",
       "         'firm': 1,\n",
       "         'strong': 1,\n",
       "         'mentally': 1,\n",
       "         'quickie': 1,\n",
       "         'juries': 1,\n",
       "         'fees': 1,\n",
       "         'minute': 1,\n",
       "         'young': 1,\n",
       "         'project': 1,\n",
       "         'considerably': 1,\n",
       "         'effected': 1,\n",
       "         'Purchasing': 1,\n",
       "         'burglary': 1,\n",
       "         'insure': 1,\n",
       "         'policeman': 1,\n",
       "         'locate': 1,\n",
       "         '1958': 1,\n",
       "         'just': 1,\n",
       "         'questioning': 1,\n",
       "         '1.5': 1,\n",
       "         'ready': 1,\n",
       "         'repeatedly': 1,\n",
       "         'extension': 1,\n",
       "         'wording': 1,\n",
       "         'sum': 1,\n",
       "         'absorbed': 1,\n",
       "         'senators': 1,\n",
       "         'complementary': 1,\n",
       "         'Hollowell': 1,\n",
       "         'attorneys': 1,\n",
       "         'However': 1,\n",
       "         'prices': 1,\n",
       "         'McLemore': 1,\n",
       "         'highly': 1,\n",
       "         'deliver': 1,\n",
       "         'Coast': 1,\n",
       "         'emphasizing': 1,\n",
       "         \"Formby's\": 1,\n",
       "         'Being': 1,\n",
       "         'Cheshire': 1,\n",
       "         'co-signers': 1,\n",
       "         '9': 1,\n",
       "         'Howard': 1,\n",
       "         'totaling': 1,\n",
       "         're-elected': 1,\n",
       "         'alternative': 1,\n",
       "         'New': 1,\n",
       "         'police': 1,\n",
       "         'subpoenas': 1,\n",
       "         'judge': 1,\n",
       "         'conducted': 1,\n",
       "         'feared': 1,\n",
       "         'undermine': 1,\n",
       "         'sessions': 1,\n",
       "         'oppose': 1,\n",
       "         'authorities': 1,\n",
       "         'projects': 1,\n",
       "         'keeping': 1,\n",
       "         'visiting': 1,\n",
       "         'absolutely': 1,\n",
       "         '250': 1,\n",
       "         'Ask': 1,\n",
       "         'sounded': 1,\n",
       "         'service': 1,\n",
       "         'desertion': 1,\n",
       "         'enact': 1,\n",
       "         'hard-fought': 1,\n",
       "         'seek': 1,\n",
       "         '4th': 1,\n",
       "         'Mac': 1,\n",
       "         'obligations': 1,\n",
       "         'attack': 1,\n",
       "         '1923': 1,\n",
       "         'equitable': 1,\n",
       "         'likely': 1,\n",
       "         'might': 1,\n",
       "         'disproportionate': 1,\n",
       "         'stays': 1,\n",
       "         'meantime': 1,\n",
       "         'wanted': 1,\n",
       "         'rally': 1,\n",
       "         'permits': 1,\n",
       "         'allowing': 1,\n",
       "         'management': 1,\n",
       "         'West': 1,\n",
       "         'eminent': 1,\n",
       "         'Asks': 1,\n",
       "         'customers': 1,\n",
       "         'accepted': 1,\n",
       "         'dissents': 1,\n",
       "         'Summerdale': 1,\n",
       "         'surveillance': 1,\n",
       "         'propose': 1,\n",
       "         'Despite': 1,\n",
       "         '$5,000': 1,\n",
       "         'nevertheless': 1,\n",
       "         'procedures': 1,\n",
       "         'Gaynor': 1,\n",
       "         '$88,000': 1,\n",
       "         'department': 1,\n",
       "         'Moreover': 1,\n",
       "         '402': 1,\n",
       "         'son': 1,\n",
       "         'rise': 1,\n",
       "         'produce': 1,\n",
       "         'benefit': 1,\n",
       "         'experimental': 1,\n",
       "         'site': 1,\n",
       "         'Only': 1,\n",
       "         'achieve': 1,\n",
       "         'feel': 1,\n",
       "         'Lt.': 1,\n",
       "         'establish': 1,\n",
       "         'Seminole': 1,\n",
       "         'Gainesville': 1,\n",
       "         'doubling': 1,\n",
       "         'team': 1,\n",
       "         '18': 1,\n",
       "         'chemistry': 1,\n",
       "         'hurdle': 1,\n",
       "         'qualified': 1,\n",
       "         'goes': 1,\n",
       "         'prevention': 1,\n",
       "         'hopper': 1,\n",
       "         'individuals': 1,\n",
       "         'East': 1,\n",
       "         'head': 1,\n",
       "         'portions': 1,\n",
       "         'Senators': 1,\n",
       "         'Bellwood': 1,\n",
       "         'pockets': 1,\n",
       "         'vocational': 1,\n",
       "         'sheriff': 1,\n",
       "         'adopted': 1,\n",
       "         'purchasing': 1,\n",
       "         'Rd.': 1,\n",
       "         'foster': 1,\n",
       "         'despite': 1,\n",
       "         'appropriation': 1,\n",
       "         '$20': 1,\n",
       "         'thanks': 1,\n",
       "         'distribution': 1,\n",
       "         'outlay': 1,\n",
       "         'lines': 1,\n",
       "         \"We're\": 1,\n",
       "         'top': 1,\n",
       "         '150': 1,\n",
       "         'protected': 1,\n",
       "         '$15,000,000': 1,\n",
       "         'latest': 1,\n",
       "         'Most': 1,\n",
       "         'influences': 1,\n",
       "         'devote': 1,\n",
       "         'administrators': 1,\n",
       "         'policies': 1,\n",
       "         'forced': 1,\n",
       "         'rather': 1,\n",
       "         'Morris': 1,\n",
       "         'storage': 1,\n",
       "         \"boy's\": 1,\n",
       "         'domain': 1,\n",
       "         '28th': 1,\n",
       "         'remedy': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597def3c",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f00e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4075a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x, y = random_batch(batch_size, news_corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "x_tensor = x_tensor.to(\"cpu\")\n",
    "y_tensor = y_tensor.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b78ffd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1548, 1521, 1859,  486,  310],\n",
       "        [ 458, 1087, 1430, 1218, 1718]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5 # number of negative samples\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)\n",
    "neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7d49681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1699])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a1fec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1548, 1521, 1859,  486,  310])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ff3771ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "\n",
    "        \n",
    "        uovc = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfdcd8",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c684723",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "811272ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocabs)\n",
    "model = SkipgramNeg(voc_size, emb_size)\n",
    "model_neg = model.to(device)\n",
    "neg_samples = neg_samples.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4bea4c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8854, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_neg(x_tensor, y_tensor, neg_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a010ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "882eea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss: 1.554515\n",
      "Epoch   2000 | Loss: 1.270081\n",
      "Epoch   3000 | Loss: 1.668319\n",
      "Epoch   4000 | Loss: 2.231380\n",
      "Epoch   5000 | Loss: 1.134095\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "\n",
    "    #move to cuda\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    label_tensor = label_tensor.to(device)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    neg_samples = neg_samples.to(device)\n",
    "\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e26a7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_neg.state_dict(), 'model/skipgram_neg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "38266cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_neg, open('model/skipgram_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eb6566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_neg_sample(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_neg.embedding_center(id_tensor)\n",
    "    u_embed = model_neg.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f2649d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_neg_sample('government')\n",
    "officials = get_embed_neg_sample('officials')\n",
    "administration = get_embed_neg_sample('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f7a56e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: -1.0000\n",
      "government vs administration: 0.6664\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9774e0",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe69ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram(window_size = 2):\n",
    "    # custom window size skip gram\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in news_corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = sent[target_index]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(sent[target_index - count])\n",
    "                context.append(sent[target_index + count])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append((target, word))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c6d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('County', 'The'),\n",
       " ('County', 'Jury'),\n",
       " ('County', 'Fulton'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Fulton'),\n",
       " ('Grand', 'said'),\n",
       " ('Grand', 'County'),\n",
       " ('Grand', 'Jury')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams = get_skipgram(2)\n",
    "skip_grams[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b9cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a72d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a794507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "        co_occer = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1 # + 1 for stability issue\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1   #count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "decc0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7da02951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1002]\n",
      " [1762]]\n",
      "Target:  [[124]\n",
      " [720]]\n",
      "Cooc:  [[1.60943791]\n",
      " [0.69314718]]\n",
      "Weighting:  [[0.10573713]\n",
      " [0.05318296]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a96270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c8b96",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3adeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "voc_size      = len(vocabs)\n",
    "device= \"cpu\"\n",
    "model_glove    = GloVe(voc_size, embedding_size)\n",
    "model_glove    = model_glove.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ff3b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 5.348379 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 1.828584 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 1.292454 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 2.378047 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 2.174004 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "\n",
    "    # to cuda\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    cooc_batch = cooc_batch.to(device)\n",
    "    weighting_batch = weighting_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "449c70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model using pickle\n",
    "pickle.dump(model_glove, open('model/glove.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7430b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_glove(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_glove.embedding_center(id_tensor)\n",
    "    u_embed = model_glove.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1dfd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_glove('government')\n",
    "officials = get_embed_glove('officials')\n",
    "administration = get_embed_glove('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "862eb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: 0.7917\n",
      "government vs administration: -0.2426\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43e3c5",
   "metadata": {},
   "source": [
    "### Glove (Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc125a52",
   "metadata": {},
   "source": [
    "Source credit: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc7d16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.100d.txt')\n",
    "model_genism = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4569d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7508\n",
      "King - Man + Woman =  queen\n"
     ]
    }
   ],
   "source": [
    "# Example: Word similarity\n",
    "similarity = model_genism.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")\n",
    "\n",
    "# Example: Word analogy\n",
    "result = model_genism.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"King - Man + Woman = \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35be4e7",
   "metadata": {},
   "source": [
    "## Task 2: Evaluating Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a26a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word1, word2, word3, embeddings, word_to_index, index_to_word):\n",
    "    # Get vectors for w1, w2, w3\n",
    "    vec1 = np.array(embeddings(word1))\n",
    "    vec2 = np.array(embeddings(word2))\n",
    "    vec3 = np.array(embeddings(word3))\n",
    "\n",
    "    # Calculate the predicted vector\n",
    "    predicted_vec = vec1 - vec2 + vec3\n",
    "\n",
    "    # Find the closest word by cosine similarity\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in [word1, word2, word3]:  # Skip the input words\n",
    "            continue\n",
    "        similarity = cos_sim(predicted_vec, embeddings(word))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8f5ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def evaluate_analogies(analogy_dataset, embeddings, word_to_index):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for analogy in analogy_dataset:\n",
    "        word1, word2, word3, word4 = analogy\n",
    "        if word1 not in word_to_index or word2 not in word_to_index or word3 not in word_to_index or word4 not in word_to_index:\n",
    "            continue  # Skip if any word is not in the vocabulary\n",
    "        predicted_word = predict_word(word1, word2, word3, embeddings, word_to_index, {v: k for k, v in word_to_index.items()})\n",
    "        if predicted_word == word4:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecff5a",
   "metadata": {},
   "source": [
    "### Syntactic and Semantic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7480aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"capital_common_countries.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "semantic_dataset = []\n",
    "for line in lines:\n",
    "    # Split the line into words\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        semantic_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc267348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['athens', 'greece', 'bangkok', 'thailand'],\n",
       " ['athens', 'greece', 'beijing', 'china'],\n",
       " ['athens', 'greece', 'berlin', 'germany'],\n",
       " ['athens', 'greece', 'bern', 'switzerland'],\n",
       " ['athens', 'greece', 'cairo', 'egypt'],\n",
       " ['athens', 'greece', 'canberra', 'australia'],\n",
       " ['athens', 'greece', 'hanoi', 'vietnam'],\n",
       " ['athens', 'greece', 'havana', 'cuba'],\n",
       " ['athens', 'greece', 'helsinki', 'finland']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_dataset[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e400b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"past-tense.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "syntactic_dataset = []\n",
    "for line in lines:\n",
    "    # Split the line into words\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        syntactic_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af56f3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dancing', 'danced', 'describing', 'described'],\n",
       " ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       " ['dancing', 'danced', 'falling', 'fell'],\n",
       " ['dancing', 'danced', 'feeding', 'fed']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_dataset[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94120b66",
   "metadata": {},
   "source": [
    "### Synctactic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b517845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(syntactic_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Syntactic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3510ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(syntactic_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Syntactic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04b77c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(syntactic_dataset, get_embed_glove, word2index)\n",
    "print(f\"Syntactic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be66fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - gensim: 55.45%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"past_tense_lines.txt\")[0]\n",
    "print(f\"Syntactic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e17f3e",
   "metadata": {},
   "source": [
    "### Semantic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9d4cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Semantic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9c57ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Semantic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5cac9880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_glove, word2index)\n",
    "print(f\"Semantic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f012678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - gensim: 93.87%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"capital.txt\")[0]\n",
    "print(f\"Semantic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f6e9e",
   "metadata": {},
   "source": [
    "| Model | Window Size | Training Loss | Training Time  | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Skipgram    | 2     | 7.98     | 5m 20s     | 0.0     | 0.0   |\n",
    "| Skipgram (Neg)   | 2     | 1.13     | 2m 15s     | 0.0     | 0.0     |\n",
    "| Glove    | 2     | 2.17    | 80s    | 0.0    | 0.0     |\n",
    "| Glove (Genism)    | -     | -     | -     | 55.45     | 93.87     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4c02b",
   "metadata": {},
   "source": [
    "### Similarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ff61453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1     Word 2  Similarity Index\n",
       "0     computer   keyboard              7.62\n",
       "1    Jerusalem     Israel              8.46\n",
       "2       planet     galaxy              8.11\n",
       "3       canyon  landscape              7.53\n",
       "4         OPEC    country              5.63\n",
       "..         ...        ...               ...\n",
       "247    rooster     voyage              0.62\n",
       "248       noon     string              0.54\n",
       "249      chord      smile              0.54\n",
       "250  professor   cucumber              0.31\n",
       "251       king    cabbage              0.23\n",
       "\n",
       "[252 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Word 1', 'Word 2', 'Similarity Index']\n",
    "\n",
    "df = pd.read_csv('wordsim_relatedness_goldstandard.txt', sep='\\t', header=None, names=columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "db765e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    word_1 = row['Word 1']\n",
    "    word_2 = row['Word 2']\n",
    "\n",
    "    try:\n",
    "        embed_1_neg_samp    = get_embed_neg_sample(word_1)\n",
    "        embed_2_neg_samp    = get_embed_neg_sample(word_2)\n",
    "        embed_1_skip_gram   = get_embed_skip_gram(word_1)\n",
    "        embed_2_skip_gram   = get_embed_skip_gram(word_2)\n",
    "        embed_1_glove       = get_embed_glove(word_1)\n",
    "        embed_2_glove       = get_embed_glove(word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Replacing missing embeddings with the embedding of '<UNK>'\n",
    "        embed_1_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_2_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_1_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_2_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_1_glove       = get_embed_glove('<UNK>')\n",
    "        embed_2_glove       = get_embed_glove('<UNK>')\n",
    "\n",
    "    # Computing dot product\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_skip_gram'] = np.dot(embed_1_skip_gram, embed_2_skip_gram)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2970bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_skip_gram</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>summer</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.039331</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>0.306405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day</td>\n",
       "      <td>dawn</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>country</td>\n",
       "      <td>citizen</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>planet</td>\n",
       "      <td>people</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>environment</td>\n",
       "      <td>ecology</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0.267446</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.159631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1     Word 2  Similarity Index  dot_product_neg_samp  \\\n",
       "0     computer   keyboard              7.62              0.267446   \n",
       "1    Jerusalem     Israel              8.46              0.267446   \n",
       "2       planet     galaxy              8.11              0.267446   \n",
       "3       canyon  landscape              7.53              0.267446   \n",
       "4         OPEC    country              5.63              0.267446   \n",
       "5          day     summer              3.94              0.039331   \n",
       "6          day       dawn              7.53              0.267446   \n",
       "7      country    citizen              7.31              0.267446   \n",
       "8       planet     people              5.75              0.267446   \n",
       "9  environment    ecology              8.81              0.267446   \n",
       "\n",
       "   dot_product_skip_gram  dot_product_glove  \n",
       "0               0.137540           0.159631  \n",
       "1               0.137540           0.159631  \n",
       "2               0.137540           0.159631  \n",
       "3               0.137540           0.159631  \n",
       "4               0.137540           0.159631  \n",
       "5               0.841837           0.306405  \n",
       "6               0.137540           0.159631  \n",
       "7               0.137540           0.159631  \n",
       "8               0.137540           0.159631  \n",
       "9               0.137540           0.159631  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b496a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Computing the Spearman correlation\n",
    "correlation_pos, _ = spearmanr(df['Similarity Index'], df['dot_product_skip_gram'])\n",
    "correlation_neg, _ = spearmanr(df['Similarity Index'], df['dot_product_neg_samp'])\n",
    "correlation_glove, _ = spearmanr(df['Similarity Index'], df['dot_product_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fbfcf26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient of Skipgram: -0.0387\n",
      "Spearman Correlation Coefficient of Skipgram with Negative Sampling: -0.0138\n",
      "Spearman Correlation Coefficient of Glove: 0.0933\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spearman Correlation Coefficient of Skipgram: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram with Negative Sampling: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc97fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 5.29\n"
     ]
    }
   ],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['Similarity Index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c2eadedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Correlation coefficient of Glove (genism): 0.50\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = model_genism.evaluate_word_pairs('wordsim_relatedness_goldstandard.txt')\n",
    "print(f\"Spearman Correlation Correlation coefficient of Glove (genism): {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78199679",
   "metadata": {},
   "source": [
    "| Model | Skipgram | NEG | GloVe | GloVe (genism) | Y_true |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| MSE    | -0.0387     | 0.0138     | 0.0933     | 0.5     | 5.29   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6563b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "        word_tensor = word_tensor.to(device)\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6950ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_whole_glove = get_embed_for_corpus(model_glove, vocabs)\n",
    "embed_whole_neg_skg = get_embed_for_corpus(model_neg, vocabs)\n",
    "embed_whole_skg = get_embed_for_corpus(model_glove, vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f7a1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/model_gensim.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model_genism, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cfce030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_skipgram_negative.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_neg_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2dd9197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_skipgram.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c85b554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_glove.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_glove, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb27a1",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "As observed, for window size of 2 - skipgram had the highest (7.60) training loss while Negative skipgram had the lowest (1.89). Glove also performed much better compared to skipgram with loss of 2.33.\n",
    "\n",
    "In case of Training time each model were trained for 5000 epoch. Skipgram took the longest time with 310s and Glove took the least amount of time with 34s. Negative sampling took 171s. \n",
    "\n",
    "All three models coded from scratch performed bad compared to Genism. This is because of the small corpus size and window size. All 3 models (Skipgram, Skipgram with negative sampling, Glove) had syantactic and semantic accuracy of 0%. This was expected because of the limitations of our corpus. Glove (Genism) on the other achieved Syntactic accuracy of 55.45% \n",
    "and Semantic Accuracy of 93.87%.\n",
    "\n",
    "Furthermore, for Spearman Correlation Coefficient - Genism outperforms other models with correlation score of `0.5`. The other 3 models showed poor correlation which suggests that predicted rankings do not closely match with ground truth. So our embeddings has poor correlation with human judgement.\n",
    "\n",
    "In conclusion, given the small corpus size, window size and embedding dimension - our 'made from scratch' models performed poorly. Given better hyperparameter tunings like embeddig dimensions, learning rate and even number of epochs, the models can be refined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478f3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
